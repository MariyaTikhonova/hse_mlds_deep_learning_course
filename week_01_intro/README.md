# Introduction in DL


## Lectures
* [Intro](https://www.youtube.com/watch?v=62sP9QKYrgI&list=PLEwK9wdS5g0qa3PIhR6HBDJD_QnrfP8Ei&index=1)
* [Backprop](https://www.youtube.com/watch?v=aSTwlPjJfso&list=PLEwK9wdS5g0qa3PIhR6HBDJD_QnrfP8Ei&index=2)


## Seminar
* seminar [slides](https://docs.google.com/presentation/d/1OKDtMxazo7nHRR8CuRkECL6hYXoonILZwymcWbU9btM/edit?usp=sharing)
* seminar [video](https://www.youtube.com/watch?v=uQJuZxiAUVA&list=PLDa1nku7NnMlRfI3jvKJ7mzYPXrHafQY5)
* YSDA backprop notebook [seminar](https://github.com/yandexdataschool/Practical_DL/blob/fall21/week01_backprop/backprop.ipynb)
* YSDA sgd notebook [seminar](https://github.com/yandexdataschool/Practical_DL/blob/fall21/week01_backprop/adapdive_sgd/adaptive_sgd.ipynb)


## Optional homework (0.5 point)
At the seminar, we implemented a simple neural network from scratch (YSDA backprop notebook). Now you need to write some modified versions of the dense layer:

1) DenseXavier - add Xavier initialisation
2) DenseL2 - add l2 regularisation
3) DenseMomentum - add optimisation with momentum
4) DenseRmsprop - add optimisation with rmsprop

Note that for MNIST you probably won't get much profit in metrics, however on more complex datasets you will probably get profit

## Other

http://playground.tensorflow.org/ - simple nn visualizer

http://neuralnetworksanddeeplearning.com/chap4.html - Universal approximation theorem 1D example

http://cs231n.github.io/optimization-2/ - cs231 backprop

https://explained.ai/matrix-calculus/ matrix-calculus in dl

https://ruder.io/optimizing-gradient-descent/index.html - Overview of gradient descent optimization algorithms

https://johnchenresearch.github.io/demon/ - Overview of gradient descent optimization algorithms

https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization - About initialization

