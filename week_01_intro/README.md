# Introduction in DL


## Lectures
* [Intro](https://www.youtube.com/watch?v=62sP9QKYrgI&list=PLEwK9wdS5g0qa3PIhR6HBDJD_QnrfP8Ei&index=1)
* [Backprop](https://www.youtube.com/watch?v=aSTwlPjJfso&list=PLEwK9wdS5g0qa3PIhR6HBDJD_QnrfP8Ei&index=2)
* [cs231 backprop](http://cs231n.github.io/optimization-2/)

## Overview of gradient descent optimization algorithms

https://ruder.io/optimizing-gradient-descent/index.html

https://johnchenresearch.github.io/demon/

## About initialization

https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization


## Seminars
* seminar [ slides](https://docs.google.com/presentation/d/1OKDtMxazo7nHRR8CuRkECL6hYXoonILZwymcWbU9btM/edit?usp=sharing)
* YSDA backprop notebook [seminar](https://github.com/yandexdataschool/Practical_DL/blob/fall21/week01_backprop/backprop.ipynb)
* YSDA sgd notebook [seminar](https://github.com/yandexdataschool/Practical_DL/blob/fall21/week01_backprop/adapdive_sgd/adaptive_sgd.ipynb)


## Other


http://playground.tensorflow.org/ - simple nn visualizer

http://neuralnetworksanddeeplearning.com/chap4.html - Universal approximation theorem 1D example

